{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing: Audio file to Spectrogram\n",
    "\n",
    "This is the second step in the data preprocessing pipeline. We convert segmented audio files into spectrogram images.\n",
    "\n",
    "**Multi-Class Support**: This notebook now processes all classes defined in the YAML configuration file. It automatically generates spectrograms for each class (e.g., anemonefish, noise, biological) and organizes them into the appropriate output directories.\n",
    "\n",
    "**Inputs**: Segmented `.wav` files from step 1 (stored in `_cache/1_generate_training_audio/{class_name}/`)\n",
    "**Outputs**: Spectrogram PNG images for each class (stored in `2_training_datasets/{dataset_version}/{class_name}/`)\n",
    "\n",
    "The preprocessing configuration is also saved to the output dataset directory for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook processes segmented audio files from multiple classes, converting them into spectrograms suitable for training machine learning models. The goal is to visualize the frequency content over time, focusing on the relevant frequency range (e.g., 0-2000Hz for anemonefish calls). \n",
    "\n",
    "All parameters are loaded from a YAML configuration file, ensuring consistency with step 1 (audio segmentation) and enabling reproducibility. The notebook automatically processes all classes defined in the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup and Imports\n",
    "\n",
    "This section imports the necessary Python libraries for audio processing, numerical operations, plotting, and file system interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Configuration\n",
    "\n",
    "**YAML-Based Configuration**: Load the same configuration file used in step 1 (audio segmentation). This ensures consistency across the preprocessing pipeline.\n",
    "\n",
    "**Important**: Update the `CONFIG_PATH` variable below to point to the same YAML config file you used in notebook 1-1.\n",
    "\n",
    "The configuration specifies:\n",
    "- Classes to process\n",
    "- Input/output directories\n",
    "- Spectrogram parameters (frequency range, FFT settings, output resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Configuration from YAML ---\n",
    "\n",
    "# !!! UPDATE THIS PATH TO MATCH THE CONFIG USED IN NOTEBOOK 1-1 !!!\n",
    "CONFIG_PATH = '/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/2_training_datasets/preprocessing_config_template.yaml'\n",
    "\n",
    "# Load configuration\n",
    "print(f\"Loading configuration from: {CONFIG_PATH}\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract configuration values\n",
    "WORKSPACE_BASE_PATH = Path(config['workspace_base_path'])\n",
    "DATASET_VERSION = config['dataset_version']\n",
    "CLASSES = config['classes']\n",
    "\n",
    "# Construct paths\n",
    "INPUT_AUDIO_BASE_DIR = Path(os.path.join(WORKSPACE_BASE_PATH, 'data', '_cache', '1_generate_training_audio'))\n",
    "OUTPUT_BASE_DIR = Path(os.path.join(WORKSPACE_BASE_PATH, 'data', '2_training_datasets', DATASET_VERSION))\n",
    "\n",
    "# Spectrogram parameters\n",
    "FMAX_HZ = config['spectrogram']['fmax_hz']\n",
    "N_FFT = config['spectrogram']['n_fft']\n",
    "HOP_LENGTH = config['spectrogram']['hop_length']\n",
    "WIDTH_PIXELS = config['spectrogram']['width_pixels']\n",
    "HEIGHT_PIXELS = config['spectrogram']['height_pixels']\n",
    "\n",
    "# Create output directory structure\n",
    "OUTPUT_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n=== Configuration Loaded ===\")\n",
    "print(f\"Dataset Version: {DATASET_VERSION}\")\n",
    "print(f\"Classes to process: {CLASSES}\")\n",
    "print(f\"Input directory: {INPUT_AUDIO_BASE_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_BASE_DIR}\")\n",
    "print(f\"\\nSpectrogram parameters:\")\n",
    "print(f\"  - Frequency range: 0-{FMAX_HZ} Hz\")\n",
    "print(f\"  - FFT size: {N_FFT}\")\n",
    "print(f\"  - Hop length: {HOP_LENGTH}\")\n",
    "print(f\"  - Output size: {WIDTH_PIXELS}x{HEIGHT_PIXELS} pixels\")\n",
    "\n",
    "# Validate input directory exists\n",
    "if not INPUT_AUDIO_BASE_DIR.exists():\n",
    "    print(f\"\\nWARNING: Input directory {INPUT_AUDIO_BASE_DIR} does not exist. Please run notebook 1-1 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function to Generate and Save Spectrogram\n",
    "\n",
    "This function takes an audio file path, loads the audio, computes its spectrogram, and saves it as a PNG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_spectrogram(audio_path, output_image_path, sr_target=None, n_fft=N_FFT, hop_length=HOP_LENGTH, fmax=FMAX_HZ):\n",
    "    \"\"\"\n",
    "    Generates a spectrogram from an audio file and saves it as an image\n",
    "    suitable for CNN input (no axes, labels, colorbar).\n",
    "\n",
    "    Args:\n",
    "        audio_path (Path or str): Path to the input audio file.\n",
    "        output_image_path (Path or str): Path to save the output spectrogram image.\n",
    "        sr_target (int, optional): Target sampling rate. If None, uses native.\n",
    "                                   Consider sr_target= (e.g., 2 * fmax, so 4000 or 8000 for fmax=2000Hz).\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        fmax (int): Maximum frequency relevant for the STFT and display.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(audio_path, sr=sr_target)\n",
    "\n",
    "        # Compute Short-Time Fourier Transform (STFT)\n",
    "        # The STFT will consider frequencies up to sr/2.\n",
    "        # We are interested in fmax, so ensure sr is adequate.\n",
    "        # If sr_target is set (e.g., 4000Hz for fmax=2000Hz), this is fine.\n",
    "        D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "        # Convert amplitude to decibels\n",
    "        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "        # Create a figure and axes with specific DPI for desired resolution\n",
    "        dpi = 100  # Dots per inch - adjust as needed\n",
    "        \n",
    "        # Convert pixels to inches for figsize\n",
    "        width_inches = WIDTH_PIXELS / dpi\n",
    "        height_inches = HEIGHT_PIXELS / dpi\n",
    "        \n",
    "        fig, ax = plt.subplots(1, figsize=(width_inches, height_inches), dpi=dpi)\n",
    "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1) # Remove padding around the plot\n",
    "\n",
    "        # Plot spectrogram\n",
    "        # We use librosa.display.specshow which is convenient, but ensure it doesn't add extra whitespace.\n",
    "        # The actual frequency range of S_db depends on sr and n_fft.\n",
    "        # We'll plot the relevant portion up to fmax.\n",
    "        img = librosa.display.specshow(S_db, sr=sr, hop_length=hop_length, x_axis=None, y_axis=None, fmax=fmax, ax=ax)\n",
    "        \n",
    "        # Ensure the y-axis is limited to fmax if specshow's fmax isn't perfectly cropping the data display.\n",
    "        # This cuts the *displayed* data, not the underlying STFT calculation.\n",
    "        num_frequency_bins = S_db.shape[0]\n",
    "        if fmax is not None and sr is not None:\n",
    "            # Calculate the bin index corresponding to fmax\n",
    "            fmax_bin = int(fmax / (sr / 2.0) * num_frequency_bins)\n",
    "            if fmax_bin < num_frequency_bins : # Ensure fmax_bin is within bounds\n",
    "                 ax.set_ylim(0, fmax_bin) # Set y-limit in terms of bins for specshow\n",
    "            # If fmax is higher than Nyquist, librosa handles it by showing up to Nyquist.\n",
    "\n",
    "        # Turn off all axes, labels, titles, colorbar\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Save the figure at the specified resolution\n",
    "        plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "        plt.close(fig) # Close the figure to free memory\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (optional, will be used in the next step)\n",
    "# test_audio_file = AUDIO_DIR / 'your_sample_audio.wav' # Replace with an actual file name\n",
    "# test_output_image = SPECTROGRAM_DIR / 'your_sample_audio_spectrogram.png'\n",
    "# if test_audio_file.exists():\n",
    "#    create_and_save_spectrogram(test_audio_file, test_output_image, fmax=2000)\n",
    "# else:\n",
    "#    print(f\"Test audio file {test_audio_file} not found. Skipping example generation here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Process Audio Files for All Classes\n",
    "\n",
    "This section processes all classes defined in the configuration:\n",
    "1. For each class, it reads audio files from `_cache/1_generate_training_audio/{class_name}/`\n",
    "2. Generates spectrograms for each audio file\n",
    "3. Saves spectrograms to `2_training_datasets/{dataset_version}/{class_name}/`\n",
    "4. Copies the preprocessing config to the output directory for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize statistics\n",
    "total_processed = 0\n",
    "total_errors = 0\n",
    "class_stats = {}\n",
    "example_spectrogram_path = None\n",
    "\n",
    "# Process each class\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing class: {class_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define input and output directories for this class\n",
    "    class_input_dir = INPUT_AUDIO_BASE_DIR / class_name\n",
    "    class_output_dir = OUTPUT_BASE_DIR / class_name\n",
    "    \n",
    "    # Create output directory\n",
    "    class_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if input directory exists\n",
    "    if not class_input_dir.exists():\n",
    "        print(f\"WARNING: Input directory not found: {class_input_dir}\")\n",
    "        print(f\"Skipping class '{class_name}'. Please run notebook 1-1 first.\")\n",
    "        continue\n",
    "    \n",
    "    # Find all audio files for this class\n",
    "    audio_files = [f for f in class_input_dir.glob('*.wav') if not f.name.startswith('.')]\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(f\"No .wav files found in {class_input_dir}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files to process\")\n",
    "    print(f\"Input:  {class_input_dir}\")\n",
    "    print(f\"Output: {class_output_dir}\")\n",
    "    \n",
    "    # Process files for this class\n",
    "    class_processed = 0\n",
    "    class_errors = 0\n",
    "    \n",
    "    for audio_file_path in tqdm(audio_files, desc=f\"Processing {class_name}\"):\n",
    "        output_filename = audio_file_path.stem + '_spectrogram.png'\n",
    "        output_image_path = class_output_dir / output_filename\n",
    "        \n",
    "        if create_and_save_spectrogram(audio_file_path, output_image_path, fmax=FMAX_HZ):\n",
    "            class_processed += 1\n",
    "            total_processed += 1\n",
    "            if example_spectrogram_path is None:  # Store first successful spectrogram\n",
    "                example_spectrogram_path = output_image_path\n",
    "        else:\n",
    "            class_errors += 1\n",
    "            total_errors += 1\n",
    "    \n",
    "    # Store class statistics\n",
    "    class_stats[class_name] = {\n",
    "        'processed': class_processed,\n",
    "        'errors': class_errors,\n",
    "        'total_files': len(audio_files)\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Processed {class_processed}/{len(audio_files)} files for '{class_name}'\")\n",
    "    if class_errors > 0:\n",
    "        print(f\"✗ Encountered {class_errors} errors\")\n",
    "\n",
    "# Copy configuration file to output directory for reproducibility\n",
    "config_output_path = OUTPUT_BASE_DIR / 'preprocessing_config.yaml'\n",
    "shutil.copy2(CONFIG_PATH, config_output_path)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Saved preprocessing config to: {config_output_path}\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset Version: {DATASET_VERSION}\")\n",
    "print(f\"Total spectrograms generated: {total_processed}\")\n",
    "if total_errors > 0:\n",
    "    print(f\"Total errors: {total_errors}\")\n",
    "\n",
    "print(f\"\\nBreakdown by class:\")\n",
    "for class_name in CLASSES:\n",
    "    if class_name in class_stats:\n",
    "        stats = class_stats[class_name]\n",
    "        print(f\"  {class_name}: {stats['processed']}/{stats['total_files']} files\")\n",
    "    else:\n",
    "        print(f\"  {class_name}: Not processed (no input files found)\")\n",
    "\n",
    "if example_spectrogram_path:\n",
    "    print(f\"\\nExample spectrogram saved at: {example_spectrogram_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Display an Example Spectrogram\n",
    "\n",
    "If spectrograms were successfully generated, this section displays the first one that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if example_spectrogram_path and example_spectrogram_path.exists():\n",
    "    print(f\"Displaying example spectrogram:\")\n",
    "    print(f\"Path: {example_spectrogram_path}\")\n",
    "    display(Image(filename=str(example_spectrogram_path), width=800))\n",
    "elif total_processed > 0:\n",
    "    print(\"Spectrograms were generated, but unable to display an example.\")\n",
    "    print(f\"Check the output directory: {OUTPUT_BASE_DIR}\")\n",
    "else:\n",
    "    print(\"No spectrograms were generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anemonefish_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Binary Spectrogram Classifier: Anemonefish vs. Noise\n",
    "\n",
    "This notebook trains a binary classification model to distinguish between spectrograms of anemonefish calls and background noise. \n",
    "\n",
    "Training data consists of two directories, one is spectograms of anemonefish, and the other is noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:57 - INFO - Logger initialized for binary_classifier\n",
      "14:53:57 - INFO - Monolith log file: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/logs/main.log\n",
      "14:53:57 - INFO - Error summary file: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/logs/error_summary.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/src')\n",
    "\n",
    "from anemonefish_acoustics.utils.logger import get_logger\n",
    "from anemonefish_acoustics.utils.utils import pretty_path\n",
    "from anemonefish_acoustics.models.hypermodels import TargetToNoiseHyperModel\n",
    "\n",
    "# Setup logging - always use get_logger to ensure proper handlers\n",
    "logging = get_logger(name='binary_classifier', workspace_root='/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:57 - INFO - TensorFlow is using the GPU!\n",
      "14:53:57 - INFO - Name: /physical_device:GPU:0, Type: GPU\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    logging.info(\"TensorFlow is using the GPU!\")\n",
    "    # You can print more details if needed\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        logging.info(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n",
    "else:\n",
    "    logging.warning(\"TensorFlow is NOT using the GPU. Training will be on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:57 - INFO - Loading configuration from: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/2_training_datasets/v2_biological/preprocessing_config_v2_biological.yaml\n",
      "14:53:57 - INFO - TensorBoard logs will be saved to: logs/experiments/target_to_noise_classifier/\n",
      "14:53:57 - INFO - Tuner logs will be saved to: Clown_Fish_Acoustics/logs/tuner/\n",
      "14:53:57 - INFO - Model checkpoints, config, and training results will be saved to: models/target_to_noise_classifier/\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "CONFIG_PATH = '/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/2_training_datasets/v2_biological/preprocessing_config_v2_biological.yaml'\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Load configuration\n",
    "logging.info(f\"Loading configuration from: {CONFIG_PATH}\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract configuration values\n",
    "WORKSPACE_BASE_PATH = Path(config['workspace_base_path'])\n",
    "DATASET_VERSION = config['dataset_version']\n",
    "CLASSES = config['classes']\n",
    "DATA_DIR = os.path.join(WORKSPACE_BASE_PATH, 'data', '2_training_datasets', DATASET_VERSION)\n",
    "\n",
    "MODEL_INPUT_SIZE = [config['spectrogram']['height_pixels'], config['spectrogram']['width_pixels'], 3]\n",
    "EPOCHS = config['epochs']\n",
    "TUNER_EPOCHS = config['tuner_epochs']\n",
    "MAX_TRIALS = config['max_trials']\n",
    "EXECUTIONS_PER_TRIAL = config['executions_per_trial']\n",
    "MODEL_SAVE_PATH = os.path.join(WORKSPACE_BASE_PATH, config['model_save_path'], config['project_name'])\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "LOGS_DIR = os.path.join(WORKSPACE_BASE_PATH, config['logs_dir'], config['project_name'])\n",
    "TUNER_LOGS_DIR = os.path.join(WORKSPACE_BASE_PATH, config['tuner_logs_dir'])\n",
    "PROJECT_NAME = config['project_name']\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(TUNER_LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Logs\n",
    "logging.info(f\"TensorBoard logs will be saved to: {pretty_path(LOGS_DIR)}\")\n",
    "logging.info(f\"Tuner logs will be saved to: {pretty_path(TUNER_LOGS_DIR)}\")\n",
    "logging.info(f\"Model checkpoints, config, and training results will be saved to: {pretty_path(MODEL_SAVE_PATH, num_dirs=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Val split & Class weight\n",
    "\n",
    "Here, we'll scan the specified directories for spectrogram images and assign labels based on their parent folder.\n",
    "- Images in `ANEMONEFISH_PATH` will be labeled as 'anemonefish' (1).\n",
    "- Images in `NOISE_PATH` will be labeled as 'noise' (0).\n",
    "---\n",
    "\n",
    "Plan:\n",
    "1. using CLASSES and DATA_DIR identify spectogram directory and list image paths for each class\n",
    "2. map X and Y dataset. Load X by loading all the images into an array. train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:57 - INFO - Found 23065 images in class 'noise' (index 0, one-hot: [1, 0, 0])\n",
      "14:53:57 - INFO - Found 704 images in class 'anemonefish' (index 1, one-hot: [0, 1, 0])\n",
      "14:53:57 - INFO - Found 7171 images in class 'biological' (index 2, one-hot: [0, 0, 1])\n",
      "14:53:57 - INFO - Total files loaded: 30940\n",
      "14:53:57 - INFO - Class weights: {0: 0.4471421345472939, 1: 14.649621212121213, 2: 1.4382001580439734}\n"
     ]
    }
   ],
   "source": [
    "# Load image file paths and corresponding class labels from directory structure\n",
    "\n",
    "# Initialize output arrays\n",
    "X_paths = []\n",
    "Y_labels = []\n",
    "Y_labels_for_weights = []  # For class weight calculation\n",
    "\n",
    "# Common image extensions\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "\n",
    "# Scan directory for subdirectories\n",
    "try:\n",
    "    items = os.listdir(DATA_DIR)\n",
    "except OSError as e:\n",
    "    logging.error(f\"Could not read directory {DATA_DIR}: {e}\")\n",
    "    items = []\n",
    "\n",
    "# Filter out hidden files and get only directories\n",
    "directories = [item for item in items \n",
    "              if not item.startswith('.') and \n",
    "              os.path.isdir(os.path.join(DATA_DIR, item))]\n",
    "\n",
    "# Process each directory that matches a class name\n",
    "for directory in directories:\n",
    "    if directory in CLASSES:\n",
    "        class_index = CLASSES.index(directory)\n",
    "        class_dir_path = os.path.join(DATA_DIR, directory)\n",
    "        \n",
    "        # Create one-hot encoded label\n",
    "        one_hot_label = [0] * len(CLASSES)\n",
    "        one_hot_label[class_index] = 1\n",
    "        \n",
    "        try:\n",
    "            # Get all files in the class directory\n",
    "            files = os.listdir(class_dir_path)\n",
    "            \n",
    "            # Filter for image files (exclude hidden/system files)\n",
    "            image_files = [f for f in files \n",
    "                          if not f.startswith('.') and \n",
    "                          not f.startswith('_') and\n",
    "                          os.path.splitext(f.lower())[1] in image_extensions]\n",
    "            \n",
    "            # Add file paths and labels\n",
    "            for image_file in image_files:\n",
    "                full_path = os.path.join(class_dir_path, image_file)\n",
    "                X_paths.append(full_path)\n",
    "                Y_labels.append(one_hot_label)  # One-hot encoded for training\n",
    "                Y_labels_for_weights.append(class_index)  # Class index for weight calculation\n",
    "                \n",
    "            logging.info(f\"Found {len(image_files)} images in class '{directory}' (index {class_index}, one-hot: {one_hot_label})\")\n",
    "            \n",
    "        except OSError as e:\n",
    "            logging.warning(f\"Could not read class directory {class_dir_path}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        logging.info(f\"Directory '{directory}' not in classes list, skipping\")\n",
    "\n",
    "logging.info(f\"Total files loaded: {len(X_paths)}\")\n",
    "\n",
    "# Calculate class weights for balanced training \n",
    "class_weights_array = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(Y_labels_for_weights),\n",
    "    y=np.array(Y_labels_for_weights))\n",
    "\n",
    "# Convert array to dictionary for tf training\n",
    "class_weights_dict = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
    "\n",
    "logging.info(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "# Assign to variables for consistency with existing code\n",
    "X_path = X_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:53:57 - INFO - X_train: 24752\n",
      "14:53:57 - INFO - X_val: 6188\n"
     ]
    }
   ],
   "source": [
    "# Split X and Y into train, val, test\n",
    "X_train_paths, X_val_paths, Y_train, Y_val = train_test_split(X_path, Y_labels, test_size=config['validation_size'], random_state=config['seed'], stratify=Y_labels)\n",
    "\n",
    "logging.info(f\"X_train: {len(X_train_paths)}\")\n",
    "logging.info(f\"X_val: {len(X_val_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create tf.data Datasets\n",
    "\n",
    "Create optimized tf.data datasets using the shared preprocessing module and calculate class weights for balanced training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, label):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    # Read file\n",
    "    image = tf.io.read_file(path)\n",
    "    \n",
    "    # Decode PNG to RGB tensor\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    \n",
    "    # Convert to float32 and normalize to [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    image = tf.ensure_shape(image, [256, 256, 3])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(X, Y):\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    tf_dataset = tf_dataset.map(\n",
    "        load_and_preprocess_image,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_dir = os.path.join(tempfile.gettempdir(), 'tensorflow_cache')\n",
    "# os.makedirs(tmp_dir, exist_ok=True)\n",
    "# tmp_dir = os.path.join(tmp_dir, 'tf_val_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_dir = tf.convert_to_tensor(tmp_dir, dtype=tf.string)\n",
    "# type(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:54:00.660414: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2025-10-28 14:54:00.660452: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-10-28 14:54:00.660456: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 14.04 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761663240.660999 20129724 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1761663240.661230 20129724 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tmp_dir = os.path.join(tempfile.gettempdir(), 'tf_val_cache')\n",
    "# os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "train_dataset = (create_tf_dataset(X_train_paths, Y_train)\n",
    "                   .shuffle(buffer_size=len(X_train_paths),\n",
    "                           seed=config['seed'],\n",
    "                           reshuffle_each_iteration=True)\n",
    "                   .batch(config['batch_size'])\n",
    "                   .cache()\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "val_dataset = (create_tf_dataset(X_val_paths, Y_val)\n",
    "                 .batch(config['batch_size'])\n",
    "                 .cache()\n",
    "                 .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:54:00 - INFO - Testing tf.data pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:54:03.935136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:54:04 - INFO - ✓ Successfully loaded batch:\n",
      "14:54:04 - INFO -   - Images shape: (64, 256, 256, 3)\n",
      "14:54:04 - INFO -   - Labels shape: (64, 3)\n",
      "14:54:04 - INFO -   - Image value range: [0.000, 0.996]\n",
      "14:54:04 - INFO -   - Images dtype: <dtype: 'float32'>\n",
      "14:54:04 - INFO -   - Labels dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# Test the dataset pipeline with a single batch\n",
    "if train_dataset is not None:\n",
    "    logging.info(\"Testing tf.data pipeline...\")\n",
    "    try:\n",
    "        sample_batch = next(iter(train_dataset.take(1)))\n",
    "        images, labels = sample_batch\n",
    "        logging.info(\"✓ Successfully loaded batch:\")\n",
    "        logging.info(f\"  - Images shape: {images.shape}\")\n",
    "        logging.info(f\"  - Labels shape: {labels.shape}\")\n",
    "        logging.info(f\"  - Image value range: [{tf.reduce_min(images):.3f}, {tf.reduce_max(images):.3f}]\")\n",
    "        logging.info(f\"  - Images dtype: {images.dtype}\")\n",
    "        logging.info(f\"  - Labels dtype: {labels.dtype}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"✗ Error testing pipeline: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define callbacks\n",
    "We'll use several Keras callbacks during training:\n",
    "- `ModelCheckpoint`: To save the best model based on validation loss.\n",
    "- `EarlyStopping`: To stop training if the validation loss doesn't improve for a certain number of epochs.\n",
    "- `ReduceLROnPlateau`: To reduce the learning rate if validation loss plateaus.\n",
    "- `TensorBoard`: To log training metrics and graphs for visualization with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:54:04 - INFO - Existing runs: ['run_2', 'run_1', 'run_3']\n",
      "14:54:04 - INFO - TensorBoard logs will be saved to: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/logs/experiments/target_to_noise_classifier/run_4\n",
      "14:54:04 - INFO - Model checkpoints will be saved to: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/models/target_to_noise_classifier/run_4\n",
      "14:54:04 - INFO - Best model will be saved as: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/models/target_to_noise_classifier/run_4/best_model.keras\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a unique directory for this training run's logs and checkpoints\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Get number of existing runs\n",
    "existing_runs = [d for d in os.listdir(LOGS_DIR) if d.startswith('run_')]\n",
    "logging.info(f\"Existing runs: {existing_runs}\")\n",
    "next_run_number = len(existing_runs) + 1\n",
    "\n",
    "run_log_dir = os.path.join(LOGS_DIR, f\"run_{next_run_number}\")\n",
    "run_checkpoint_dir = os.path.join(MODEL_SAVE_PATH, f\"run_{next_run_number}\")\n",
    "\n",
    "os.makedirs(run_log_dir, exist_ok=True)\n",
    "os.makedirs(run_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "best_model_path = os.path.join(run_checkpoint_dir, \"best_model.keras\") # Using .keras format\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=run_log_dir, histogram_freq=0)\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=best_model_path,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss', # Save the model with the best validation loss\n",
    "    mode='min',         # The lower the validation loss, the better\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50, # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=5,  # Number of epochs with no improvement after which learning rate will be reduced.\n",
    "    min_lr=1e-6, # Lower bound on the learning rate.\n",
    "    verbose=1\n",
    ")\n",
    "callbacks_list = [\n",
    "    tensorboard_callback,\n",
    "    model_checkpoint_callback,\n",
    "    early_stopping_callback,\n",
    "    reduce_lr_callback\n",
    "]\n",
    "callbacks_list_tuner = [\n",
    "    tensorboard_callback,\n",
    "    reduce_lr_callback\n",
    "]\n",
    "\n",
    "logging.info(f\"TensorBoard logs will be saved to: {run_log_dir}\")\n",
    "logging.info(f\"Model checkpoints will be saved to: {run_checkpoint_dir}\")\n",
    "logging.info(f\"Best model will be saved as: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 train and tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Start the search (tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the hypermodel\n",
    "hypermodel = TargetToNoiseHyperModel(input_shape=MODEL_INPUT_SIZE, num_classes=len(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/logs/tuner/target_to_noise_classifier/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid'], 'ordered': False}\n",
      "cnn_depth (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 6, 'step': 1, 'sampling': 'linear'}\n",
      "use_dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=hypermodel,\n",
    "    objective='val_loss',\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
    "    directory=TUNER_LOGS_DIR,\n",
    "    project_name=PROJECT_NAME\n",
    ")\n",
    "tuner.search_space_summary(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "sigmoid           |relu              |activation\n",
      "5                 |1                 |cnn_depth\n",
      "False             |True              |use_dropout\n",
      "0.00039186        |0.00028224        |learning_rate\n",
      "0.4               |0.5               |dropout_rate\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 14:54:15.037067: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 1s/step - accuracy: 0.3636 - loss: 1.1043 - val_accuracy: 0.0228 - val_loss: 1.2583 - learning_rate: 3.9186e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 1s/step - accuracy: 0.3061 - loss: 1.0990 - val_accuracy: 0.0228 - val_loss: 1.1117 - learning_rate: 3.9186e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 1s/step - accuracy: 0.3007 - loss: 1.0989 - val_accuracy: 0.0228 - val_loss: 1.1036 - learning_rate: 3.9186e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 1s/step - accuracy: 0.2983 - loss: 1.0989 - val_accuracy: 0.0228 - val_loss: 1.1030 - learning_rate: 3.9186e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m386/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3067 - loss: 1.0883"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tuner.search(train_dataset, epochs=TUNER_EPOCHS, validation_data=val_dataset, callbacks=callbacks_list_tuner, class_weight=class_weights_dict)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during tuning.search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 query the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)\n",
    "model = models[0]\n",
    "model.summary()\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = build_model_target_to_noise(best_hps)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "logging.info(\"Saving the trained model...\")\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Save the model in Keras format (.keras)\n",
    "model_save_file = os.path.join(MODEL_SAVE_PATH, 'model.keras')\n",
    "model.save(model_save_file)\n",
    "logging.info(f\"Model saved to: {model_save_file}\")\n",
    "\n",
    "# Also save model weights separately (optional)\n",
    "weights_save_file = os.path.join(MODEL_SAVE_PATH, 'weights.weights.h5')\n",
    "model.save_weights(weights_save_file)\n",
    "logging.info(f\"Model weights saved to: {weights_save_file}\")\n",
    "\n",
    "# Save training history\n",
    "history_save_file = os.path.join(MODEL_SAVE_PATH, 'training_history.npy')\n",
    "np.save(history_save_file, history.history)\n",
    "logging.info(f\"Training history saved to: {history_save_file}\")\n",
    "\n",
    "# Save model configuration for reference\n",
    "model_config = {\n",
    "    'input_shape': MODEL_INPUT_SIZE,\n",
    "    'num_classes': len(CLASSES),\n",
    "    'classes': CLASSES,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'final_train_loss': history.history['loss'][-1],\n",
    "    'final_val_loss': history.history['val_loss'][-1],\n",
    "    'final_train_accuracy': history.history['accuracy'][-1],\n",
    "    'final_val_accuracy': history.history['val_accuracy'][-1]\n",
    "}\n",
    "\n",
    "config_save_file = os.path.join(MODEL_SAVE_PATH, 'model_config.yaml')\n",
    "with open(config_save_file, 'w') as f:\n",
    "    yaml.dump(model_config, f, default_flow_style=False)\n",
    "logging.info(f\"Model configuration saved to: {config_save_file}\")\n",
    "\n",
    "logging.info(\"✅ Model saving completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate the Model\n",
    "\n",
    "After training, we'll evaluate the model's performance on the unseen test set.\n",
    "We will:\n",
    "- Load the best weights saved during training (if `restore_best_weights=True` in `EarlyStopping`, this is already done).\n",
    "- Make predictions on the test set.\n",
    "- Calculate and display key metrics like accuracy, precision, recall, F1-score, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If EarlyStopping with restore_best_weights=True was used, \n",
    "# # the model already has the best weights. Otherwise, load them:\n",
    "# if os.path.exists(best_model_path):\n",
    "#     logging.info(f\"Loading best model weights from: {best_model_path}\")\n",
    "#     model.load_weights(best_model_path)\n",
    "# else:\n",
    "#     logging.warning(\"Best model checkpoint not found. Evaluating with current model weights.\")\n",
    "\n",
    "# if test_dataset is not None:\n",
    "#     logging.info(\"Evaluating model on the test set using tf.data...\")\n",
    "    \n",
    "#     # Use the test dataset directly for prediction and evaluation\n",
    "#     y_pred_probs = model.predict(test_dataset, verbose=1)\n",
    "#     y_pred_test = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "    \n",
    "#     # Get true labels (we already have them from the data split)\n",
    "#     y_true_test = test_labels\n",
    "    \n",
    "#     # Calculate metrics using the test dataset\n",
    "#     test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_dataset, verbose=0)\n",
    "    \n",
    "#     logging.info(f\"Test Loss: {test_loss:.4f}\")\n",
    "#     logging.info(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "#     logging.info(f\"Test Precision: {test_precision:.4f}\")\n",
    "#     logging.info(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "#     logging.info(\"Classification Report on Test Set:\")\n",
    "#     logging.info(f\"\\n{classification_report(y_true_test, y_pred_test, target_names=CLASS_NAMES)}\")\n",
    "\n",
    "#     logging.info(\"Confusion Matrix on Test Set:\")\n",
    "#     cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "#     logging.info(f\"\\n{cm}\")\n",
    "\n",
    "#     # Plotting the confusion matrix\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "#     fig.colorbar(cax)\n",
    "#     ax.set_xlabel('Predicted Labels')\n",
    "#     ax.set_ylabel('True Labels')\n",
    "#     ax.set_title('Confusion Matrix')\n",
    "#     ax.xaxis.set_ticklabels([''] + CLASS_NAMES) # Add empty string for 0-tick\n",
    "#     ax.yaxis.set_ticklabels([''] + CLASS_NAMES) # Add empty string for 0-tick\n",
    "    \n",
    "#     # Annotate cells with counts\n",
    "#     for i in range(cm.shape[0]):\n",
    "#         for j in range(cm.shape[1]):\n",
    "#             ax.text(j, i, str(cm[i, j]), va='center', ha='center', color='black' if cm[i,j] < (cm.max()/2) else 'white')\n",
    "            \n",
    "#     plt.show()\n",
    "\n",
    "# else:\n",
    "#     logging.warning(\"Test dataset is not available. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Training History\n",
    "\n",
    "Plotting the training and validation accuracy and loss helps to understand the model's learning process and identify potential issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' in locals() and history is not None:\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history.get('val_accuracy') # Use .get() in case validation was skipped\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss') # Use .get()\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Training and Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    if val_acc:\n",
    "        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    if val_loss:\n",
    "        plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.warning(\"Training history not available. Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save the Final Model (Optional)\n",
    "\n",
    "The `ModelCheckpoint` callback already saved the best performing model during training.\n",
    "This step is to explicitly save the model's final state (which might be different from the best if `restore_best_weights=False` or if you continued training after early stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tf2onnx\n",
    "# import tensorflow as tf # Required for tf.TensorSpec and if 'model' is tf.keras.Model\n",
    "\n",
    "# # It's assumed that 'model', 'best_model_path', 'MODEL_SAVE_PATH', \n",
    "# # and 'current_time' are defined in previous cells of your notebook.\n",
    "\n",
    "# # The best model is already saved by ModelCheckpoint (likely in Keras format)\n",
    "# logging.info(f\"The best performing Keras model (from ModelCheckpoint) was saved to: {best_model_path}\")\n",
    "\n",
    "# # Define the path for the ONNX model\n",
    "# # This uses the directory from MODEL_SAVE_PATH and the current_time string, similar to original logic\n",
    "# onnx_model_dir = os.path.dirname(run_checkpoint_dir)\n",
    "# onnx_model_filename = f\"model.onnx\"\n",
    "# onnx_model_save_path = os.path.join(onnx_model_dir, onnx_model_filename)\n",
    "\n",
    "# logging.info(f\"Preparing to save the final model in ONNX format to: {onnx_model_save_path}\")\n",
    "\n",
    "# try:\n",
    "#     # Convert the Keras model to ONNX.\n",
    "#     # 'model' should be your trained tf.keras.Model instance.\n",
    "    \n",
    "#     # For many common models, tf2onnx can infer the input signature.\n",
    "#     # If conversion fails, you may need to explicitly provide the input_signature.\n",
    "#     # ----- Example for explicitly defining input_signature -----\n",
    "#     # # Replace (None, height, width, channels) with your model's actual input shape and dtype.\n",
    "#     # # For a model with input shape (e.g., 128, 128, 1) for spectrograms:\n",
    "#     # input_signature = [tf.TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=\"input_spectrogram\")]\n",
    "#     #\n",
    "#     # # If your model has multiple inputs, provide a list of tf.TensorSpec objects.\n",
    "#     # # You can also try to derive it dynamically from the model (might need adjustments):\n",
    "#     # # if hasattr(model, 'inputs') and model.inputs:\n",
    "#     # #     input_signature = [tf.TensorSpec.from_tensor(tensor) for tensor in model.inputs]\n",
    "#     # # else:\n",
    "#     # #     logging.info(\"Could not automatically determine input signature from model.inputs. You may need to define it manually.\")\n",
    "#     # #     input_signature = None # Fallback to tf2onnx inference\n",
    "#     # ----- End of example -----\n",
    "\n",
    "#     # For now, we'll let tf2onnx try to infer the input signature.\n",
    "#     # If this fails, define 'input_signature' using the examples above.\n",
    "#     input_signature = None \n",
    "\n",
    "#     logging.info(\"Starting Keras to ONNX conversion...\")\n",
    "#     # Ensure the 'model' variable holds your trained Keras model\n",
    "#     model_proto, external_tensor_storage = tf2onnx.convert.from_keras(\n",
    "#         model=model,\n",
    "#         input_signature=input_signature,\n",
    "#         opset=13,  # Opset 13 is a common choice; adjust if needed for compatibility\n",
    "#         output_path=onnx_model_save_path\n",
    "#     )\n",
    "#     logging.info(f\"Successfully saved model in ONNX format to: {onnx_model_save_path}\")\n",
    "\n",
    "# except ImportError:\n",
    "#     logging.error(\"The 'tf2onnx' library was not found.\")\n",
    "#     logging.error(\"Please install it, for example, by running: pip install tf2onnx\")\n",
    "# except AttributeError as ae:\n",
    "#     if 'model' in str(ae):\n",
    "#         logging.error(\"The 'model' variable is likely not defined or is not a Keras model.\")\n",
    "#         logging.error(\"Ensure 'model' is your trained Keras model instance before this cell.\")\n",
    "#     else:\n",
    "#         logging.error(f\"An AttributeError occurred: {ae}\")\n",
    "#         logging.error(\"This might be due to an issue with the model structure or tf2onnx.\")\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"An error occurred during Keras to ONNX conversion: {e}\")\n",
    "#     logging.error(\"Tips for troubleshooting:\")\n",
    "#     logging.error(\"- Ensure 'tf2onnx' and its dependencies (like 'onnx') are installed and up to date (`pip install -U tf2onnx onnx`).\")\n",
    "#     logging.error(\"- If the error mentions input shapes, types, or names, you most likely need to define the 'input_signature' argument for `tf2onnx.convert.from_keras` explicitly.\")\n",
    "#     logging.error(\"  See the commented-out 'Example for explicitly defining input_signature' in the code above.\")\n",
    "#     logging.error(\"  Adjust the shape (e.g., `(None, 128, 128, 1)`), `dtype` (e.g., `tf.float32`), and `name` to match your model's input layer(s).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anemonefish_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

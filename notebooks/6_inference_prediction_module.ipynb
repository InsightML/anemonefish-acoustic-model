{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anemonefish Call Prediction Module\n",
        "\n",
        "This notebook implements a prediction module that processes audio files to detect anemonefish calls using the trained binary classifier. The module applies the exact same preprocessing pipeline as training and uses a sliding window approach for temporal analysis.\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Audio Loading & Preprocessing** - Load and normalize audio consistently with training\n",
        "2. **Sliding Window Analysis** - Extract 1-second windows with overlap for high temporal resolution\n",
        "3. **Spectrogram Generation** - Create spectrograms with identical parameters as training\n",
        "4. **Model Prediction** - Apply trained classifier to each window\n",
        "5. **Post-Processing** - Smooth predictions and detect events\n",
        "6. **Results Export** - Generate timestamps and confidence scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.2\n",
            "GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import json\n",
        "from scipy import ndimage\n",
        "from scipy.signal import medfilt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "All parameters must match the training setup exactly to ensure consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  Model: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/models/binary_classifier/checkpoints_run_6/best_model.keras\n",
            "  Input Directory: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/prediction_input\n",
            "  Output Directory: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/results/predictions\n",
            "  Window Duration: 1.0s\n",
            "  Stride Duration: 0.1s\n",
            "  Probability Threshold: 0.5\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIGURATION ---\n",
        "\n",
        "# Model and paths\n",
        "MODEL_PATH = '/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/models/binary_classifier/checkpoints_run_6/best_model.keras'\n",
        "AUDIO_INPUT_DIR = Path('/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/prediction_input')  # Input audio files\n",
        "RESULTS_OUTPUT_DIR = Path('/Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/results/predictions')  # Output results\n",
        "\n",
        "# Spectrogram Parameters (MUST match training exactly)\n",
        "SPECTROGRAM_CONFIG = {\n",
        "    'n_fft': 1024,\n",
        "    'hop_length': 256,  # n_fft // 4\n",
        "    'fmax': 2000,  # Max frequency for anemonefish calls\n",
        "    'width_pixels': 256,\n",
        "    'height_pixels': 256,\n",
        "    'target_sr': None,  # Use original sample rate\n",
        "    'normalization_mean': [0.485, 0.456, 0.406],  # ImageNet normalization\n",
        "    'normalization_std': [0.229, 0.224, 0.225]\n",
        "}\n",
        "\n",
        "# Sliding Window Parameters\n",
        "WINDOW_CONFIG = {\n",
        "    'window_duration': 1.0,  # seconds (matches training)\n",
        "    'stride_duration': 0.1,  # seconds (high temporal resolution)\n",
        "    'min_duration': 0.1     # minimum segment duration to process\n",
        "}\n",
        "\n",
        "# Prediction Parameters\n",
        "PREDICTION_CONFIG = {\n",
        "    'batch_size': 32,\n",
        "    'probability_threshold': 0.5,\n",
        "    'min_event_duration': 0.2,  # seconds\n",
        "    'min_gap_duration': 0.1,    # seconds between events\n",
        "    'smoothing_window': 5       # for median filtering\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "RESULTS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "AUDIO_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Model: {MODEL_PATH}\")\n",
        "print(f\"  Input Directory: {AUDIO_INPUT_DIR}\")\n",
        "print(f\"  Output Directory: {RESULTS_OUTPUT_DIR}\")\n",
        "print(f\"  Window Duration: {WINDOW_CONFIG['window_duration']}s\")\n",
        "print(f\"  Stride Duration: {WINDOW_CONFIG['stride_duration']}s\")\n",
        "print(f\"  Probability Threshold: {PREDICTION_CONFIG['probability_threshold']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Audio Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_audio(audio_path, target_sr=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess audio file consistently with training pipeline.\n",
        "    \n",
        "    Args:\n",
        "        audio_path: Path to audio file\n",
        "        target_sr: Target sample rate (None to use original)\n",
        "    \n",
        "    Returns:\n",
        "        audio_data: Preprocessed audio array\n",
        "        sample_rate: Sample rate of audio\n",
        "        duration: Duration in seconds\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio using librosa (matches training pipeline)\n",
        "        audio_data, sample_rate = librosa.load(audio_path, sr=target_sr)\n",
        "        \n",
        "        # Ensure mono\n",
        "        if audio_data.ndim > 1:\n",
        "            audio_data = np.mean(audio_data, axis=0)\n",
        "        \n",
        "        duration = len(audio_data) / sample_rate\n",
        "        \n",
        "        print(f\"Loaded audio: {duration:.2f}s at {sample_rate}Hz\")\n",
        "        return audio_data, sample_rate, duration\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio {audio_path}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "def generate_sliding_windows(audio_data, sample_rate, window_duration, stride_duration):\n",
        "    \"\"\"\n",
        "    Generate sliding window parameters for audio processing.\n",
        "    \n",
        "    Args:\n",
        "        audio_data: Audio array\n",
        "        sample_rate: Sample rate\n",
        "        window_duration: Window size in seconds\n",
        "        stride_duration: Stride size in seconds\n",
        "    \n",
        "    Returns:\n",
        "        windows: List of (start_sample, end_sample, start_time, end_time) tuples\n",
        "    \"\"\"\n",
        "    window_samples = int(window_duration * sample_rate)\n",
        "    stride_samples = int(stride_duration * sample_rate)\n",
        "    total_samples = len(audio_data)\n",
        "    \n",
        "    windows = []\n",
        "    \n",
        "    start_sample = 0\n",
        "    while start_sample + window_samples <= total_samples:\n",
        "        end_sample = start_sample + window_samples\n",
        "        start_time = start_sample / sample_rate\n",
        "        end_time = end_sample / sample_rate\n",
        "        \n",
        "        windows.append((start_sample, end_sample, start_time, end_time))\n",
        "        start_sample += stride_samples\n",
        "    \n",
        "    print(f\"Generated {len(windows)} windows with {window_duration:.2f}s window, {stride_duration:.2f}s stride\")\n",
        "    return windows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Spectrogram Generation (Training-Identical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_spectrogram_for_model(audio_segment, sample_rate, config):\n",
        "    \"\"\"\n",
        "    Create spectrogram identical to training pipeline.\n",
        "    This function replicates the exact spectrogram generation from notebook 1.\n",
        "    \n",
        "    Args:\n",
        "        audio_segment: 1D audio array\n",
        "        sample_rate: Sample rate\n",
        "        config: Spectrogram configuration dictionary\n",
        "    \n",
        "    Returns:\n",
        "        spectrogram: Preprocessed spectrogram ready for model input (H, W, C)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Compute STFT (matches training)\n",
        "        D = librosa.stft(audio_segment, \n",
        "                        n_fft=config['n_fft'], \n",
        "                        hop_length=config['hop_length'])\n",
        "        \n",
        "        # Convert to dB scale (matches training)\n",
        "        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "        \n",
        "        # Create figure for rendering (matches training approach)\n",
        "        dpi = 100\n",
        "        width_inches = config['width_pixels'] / dpi\n",
        "        height_inches = config['height_pixels'] / dpi\n",
        "        \n",
        "        fig, ax = plt.subplots(1, figsize=(width_inches, height_inches), dpi=dpi)\n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "        \n",
        "        # Plot spectrogram\n",
        "        librosa.display.specshow(S_db, sr=sample_rate, \n",
        "                               hop_length=config['hop_length'],\n",
        "                               x_axis=None, y_axis=None, \n",
        "                               fmax=config['fmax'], ax=ax)\n",
        "        \n",
        "        # Handle frequency limit\n",
        "        num_frequency_bins = S_db.shape[0]\n",
        "        if config['fmax'] is not None and sample_rate is not None:\n",
        "            fmax_bin = int(config['fmax'] / (sample_rate / 2.0) * num_frequency_bins)\n",
        "            if fmax_bin < num_frequency_bins:\n",
        "                ax.set_ylim(0, fmax_bin)\n",
        "        \n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Convert to array (fix for newer matplotlib versions)\n",
        "        fig.canvas.draw()\n",
        "        buf = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
        "        buf = buf.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "        # Convert RGBA to RGB\n",
        "        buf = buf[:, :, :3]\n",
        "        plt.close(fig)\n",
        "        \n",
        "        # Resize to exact dimensions\n",
        "        spectrogram = cv2.resize(buf, (config['width_pixels'], config['height_pixels']))\n",
        "        \n",
        "        # Normalize (matches training)\n",
        "        spectrogram = spectrogram.astype(np.float32) / 255.0\n",
        "        mean = np.array(config['normalization_mean'])\n",
        "        std = np.array(config['normalization_std'])\n",
        "        spectrogram = (spectrogram - mean) / std\n",
        "        \n",
        "        return spectrogram\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating spectrogram: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_windows_to_spectrograms(audio_data, windows, sample_rate, config):\n",
        "    \"\"\"\n",
        "    Process all windows to create spectrograms for model input.\n",
        "    \n",
        "    Args:\n",
        "        audio_data: Full audio array\n",
        "        windows: List of window parameters\n",
        "        sample_rate: Sample rate\n",
        "        config: Spectrogram configuration\n",
        "    \n",
        "    Returns:\n",
        "        spectrograms: Array of spectrograms (N, H, W, C)\n",
        "        timestamps: List of (start_time, end_time) for each window\n",
        "    \"\"\"\n",
        "    spectrograms = []\n",
        "    timestamps = []\n",
        "    \n",
        "    print(f\"Processing {len(windows)} windows to spectrograms...\")\n",
        "    \n",
        "    for start_sample, end_sample, start_time, end_time in tqdm(windows):\n",
        "        # Extract audio segment\n",
        "        audio_segment = audio_data[start_sample:end_sample]\n",
        "        \n",
        "        # Create spectrogram\n",
        "        spectrogram = create_spectrogram_for_model(audio_segment, sample_rate, config)\n",
        "        \n",
        "        if spectrogram is not None:\n",
        "            spectrograms.append(spectrogram)\n",
        "            timestamps.append((start_time, end_time))\n",
        "    \n",
        "    if spectrograms:\n",
        "        spectrograms = np.array(spectrograms)\n",
        "        print(f\"Created {len(spectrograms)} spectrograms with shape: {spectrograms.shape}\")\n",
        "    else:\n",
        "        spectrograms = np.array([])\n",
        "        print(\"No spectrograms created\")\n",
        "    \n",
        "    return spectrograms, timestamps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Loading and Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the trained binary classifier model.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to the trained model\n",
        "    \n",
        "    Returns:\n",
        "        model: Loaded Keras model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Model loaded successfully from: {model_path}\")\n",
        "        print(f\"Model input shape: {model.input_shape}\")\n",
        "        print(f\"Model output shape: {model.output_shape}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def predict_batch(model, spectrograms, batch_size=32):\n",
        "    \"\"\"\n",
        "    Run batch prediction on spectrograms.\n",
        "    \n",
        "    Args:\n",
        "        model: Loaded Keras model\n",
        "        spectrograms: Array of spectrograms\n",
        "        batch_size: Batch size for prediction\n",
        "    \n",
        "    Returns:\n",
        "        predictions: Array of probability scores\n",
        "    \"\"\"\n",
        "    if len(spectrograms) == 0:\n",
        "        return np.array([])\n",
        "    \n",
        "    print(f\"Running prediction on {len(spectrograms)} spectrograms...\")\n",
        "    \n",
        "    try:\n",
        "        # Predict in batches\n",
        "        predictions = model.predict(spectrograms, batch_size=batch_size, verbose=1)\n",
        "        \n",
        "        # Flatten predictions (from (N, 1) to (N,))\n",
        "        predictions = predictions.flatten()\n",
        "        \n",
        "        print(f\"Predictions shape: {predictions.shape}\")\n",
        "        print(f\"Prediction range: {predictions.min():.3f} - {predictions.max():.3f}\")\n",
        "        \n",
        "        return predictions\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return np.array([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Post-Processing and Event Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth_predictions(predictions, window_size=5):\n",
        "    \"\"\"\n",
        "    Apply median filtering to smooth predictions.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Array of prediction scores\n",
        "        window_size: Size of median filter window\n",
        "    \n",
        "    Returns:\n",
        "        smoothed_predictions: Smoothed prediction array\n",
        "    \"\"\"\n",
        "    if len(predictions) == 0:\n",
        "        return predictions\n",
        "    \n",
        "    # Apply median filter\n",
        "    smoothed = medfilt(predictions, kernel_size=window_size)\n",
        "    \n",
        "    print(f\"Applied median filtering with window size {window_size}\")\n",
        "    return smoothed\n",
        "\n",
        "\n",
        "def detect_events(predictions, timestamps, config):\n",
        "    \"\"\"\n",
        "    Detect events from prediction stream using thresholding and post-processing.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Array of prediction scores\n",
        "        timestamps: List of (start_time, end_time) for each prediction\n",
        "        config: Prediction configuration\n",
        "    \n",
        "    Returns:\n",
        "        events: List of detected events with start/end times and confidence\n",
        "    \"\"\"\n",
        "    if len(predictions) == 0:\n",
        "        return []\n",
        "    \n",
        "    threshold = config['probability_threshold']\n",
        "    min_event_duration = config['min_event_duration']\n",
        "    min_gap_duration = config['min_gap_duration']\n",
        "    \n",
        "    # Apply threshold\n",
        "    binary_predictions = predictions > threshold\n",
        "    \n",
        "    # Find event boundaries\n",
        "    events = []\n",
        "    in_event = False\n",
        "    event_start = None\n",
        "    event_confidences = []\n",
        "    \n",
        "    for i, (is_event, (start_time, end_time)) in enumerate(zip(binary_predictions, timestamps)):\n",
        "        if is_event and not in_event:\n",
        "            # Start of new event\n",
        "            in_event = True\n",
        "            event_start = start_time\n",
        "            event_confidences = [predictions[i]]\n",
        "            \n",
        "        elif is_event and in_event:\n",
        "            # Continue current event\n",
        "            event_confidences.append(predictions[i])\n",
        "            \n",
        "        elif not is_event and in_event:\n",
        "            # End of current event\n",
        "            in_event = False\n",
        "            event_end = timestamps[i-1][1] if i > 0 else end_time\n",
        "            event_duration = event_end - event_start\n",
        "            \n",
        "            # Check minimum duration\n",
        "            if event_duration >= min_event_duration:\n",
        "                mean_confidence = np.mean(event_confidences)\n",
        "                max_confidence = np.max(event_confidences)\n",
        "                \n",
        "                events.append({\n",
        "                    'start_time': event_start,\n",
        "                    'end_time': event_end,\n",
        "                    'duration': event_duration,\n",
        "                    'mean_confidence': mean_confidence,\n",
        "                    'max_confidence': max_confidence\n",
        "                })\n",
        "    \n",
        "    # Handle case where file ends during an event\n",
        "    if in_event and event_start is not None:\n",
        "        event_end = timestamps[-1][1]\n",
        "        event_duration = event_end - event_start\n",
        "        \n",
        "        if event_duration >= min_event_duration:\n",
        "            mean_confidence = np.mean(event_confidences)\n",
        "            max_confidence = np.max(event_confidences)\n",
        "            \n",
        "            events.append({\n",
        "                'start_time': event_start,\n",
        "                'end_time': event_end,\n",
        "                'duration': event_duration,\n",
        "                'mean_confidence': mean_confidence,\n",
        "                'max_confidence': max_confidence\n",
        "            })\n",
        "    \n",
        "    # Merge events that are too close together\n",
        "    if len(events) > 1:\n",
        "        merged_events = []\n",
        "        current_event = events[0]\n",
        "        \n",
        "        for next_event in events[1:]:\n",
        "            gap = next_event['start_time'] - current_event['end_time']\n",
        "            \n",
        "            if gap < min_gap_duration:\n",
        "                # Merge events\n",
        "                current_event['end_time'] = next_event['end_time']\n",
        "                current_event['duration'] = current_event['end_time'] - current_event['start_time']\n",
        "                current_event['mean_confidence'] = (current_event['mean_confidence'] + next_event['mean_confidence']) / 2\n",
        "                current_event['max_confidence'] = max(current_event['max_confidence'], next_event['max_confidence'])\n",
        "            else:\n",
        "                # Keep separate\n",
        "                merged_events.append(current_event)\n",
        "                current_event = next_event\n",
        "        \n",
        "        merged_events.append(current_event)\n",
        "        events = merged_events\n",
        "    \n",
        "    print(f\"Detected {len(events)} events after post-processing\")\n",
        "    return events\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Results Export and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_results(events, predictions, timestamps, audio_filename, output_dir):\n",
        "    \"\"\"\n",
        "    Save prediction results in multiple formats.\n",
        "    \n",
        "    Args:\n",
        "        events: List of detected events\n",
        "        predictions: Array of all prediction scores\n",
        "        timestamps: List of (start_time, end_time) for predictions\n",
        "        audio_filename: Name of processed audio file\n",
        "        output_dir: Output directory path\n",
        "    \n",
        "    Returns:\n",
        "        result_files: Dictionary of created result files\n",
        "    \"\"\"\n",
        "    base_name = Path(audio_filename).stem\n",
        "    result_files = {}\n",
        "    \n",
        "    # 1. Save events as CSV\n",
        "    if events:\n",
        "        events_df = pd.DataFrame(events)\n",
        "        events_csv_path = output_dir / f\"{base_name}_events.csv\"\n",
        "        events_df.to_csv(events_csv_path, index=False)\n",
        "        result_files['events_csv'] = events_csv_path\n",
        "        print(f\"Events saved to: {events_csv_path}\")\n",
        "    \n",
        "    # 2. Save events as Audacity labels\n",
        "    if events:\n",
        "            audacity_path = output_dir / f\"{base_name}_audacity_labels.txt\"\n",
        "            with open(audacity_path, 'w') as f:\n",
        "                for event in events:\n",
        "                    f.write(f\"{event['start_time']:.3f}\\t{event['end_time']:.3f}\\tanemonefish_call\\n\")\n",
        "            result_files['audacity_labels'] = audacity_path\n",
        "            print(f\"Audacity labels saved to: {audacity_path}\")\n",
        "    \n",
        "    # 3. Save all predictions as CSV\n",
        "    if len(predictions) > 0:\n",
        "        pred_data = []\n",
        "        for i, (pred, (start_time, end_time)) in enumerate(zip(predictions, timestamps)):\n",
        "            pred_data.append({\n",
        "                'window_index': i,\n",
        "                'start_time': start_time,\n",
        "                'end_time': end_time,\n",
        "                'center_time': (start_time + end_time) / 2,\n",
        "                'probability': pred\n",
        "            })\n",
        "        \n",
        "        pred_df = pd.DataFrame(pred_data)\n",
        "        pred_csv_path = output_dir / f\"{base_name}_predictions.csv\"\n",
        "        pred_df.to_csv(pred_csv_path, index=False)\n",
        "        result_files['predictions_csv'] = pred_csv_path\n",
        "        print(f\"All predictions saved to: {pred_csv_path}\")\n",
        "    \n",
        "    # 4. Save summary JSON\n",
        "    summary = {\n",
        "        'audio_file': audio_filename,\n",
        "        'processing_time': timestamps[-1][1] if timestamps else 0,\n",
        "        'total_windows': len(predictions),\n",
        "        'total_events': len(events),\n",
        "        'events_summary': []\n",
        "    }\n",
        "    \n",
        "    for i, event in enumerate(events):\n",
        "        summary['events_summary'].append({\n",
        "            'event_id': i + 1,\n",
        "            'start_time': event['start_time'],\n",
        "            'end_time': event['end_time'],\n",
        "            'duration': event['duration'],\n",
        "            'mean_confidence': event['mean_confidence'],\n",
        "            'max_confidence': event['max_confidence']\n",
        "        })\n",
        "    \n",
        "    summary_json_path = output_dir / f\"{base_name}_summary.json\"\n",
        "    with open(summary_json_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    result_files['summary_json'] = summary_json_path\n",
        "    print(f\"Summary saved to: {summary_json_path}\")\n",
        "    \n",
        "    return result_files\n",
        "\n",
        "\n",
        "def plot_prediction_timeline(predictions, timestamps, events, audio_filename, output_dir):\n",
        "    \"\"\"\n",
        "    Create visualization of prediction timeline.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Array of prediction scores\n",
        "        timestamps: List of (start_time, end_time)\n",
        "        events: List of detected events\n",
        "        audio_filename: Name of audio file\n",
        "        output_dir: Output directory\n",
        "    \"\"\"\n",
        "    if len(predictions) == 0:\n",
        "        return\n",
        "    \n",
        "    # Create time array from timestamps\n",
        "    time_points = np.array([t[0] for t in timestamps])\n",
        "    \n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Plot prediction scores\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(time_points, predictions, 'b-', alpha=0.7, label='Prediction Scores')\n",
        "    plt.axhline(y=PREDICTION_CONFIG['probability_threshold'], color='r', linestyle='--', \n",
        "                label=f'Threshold {PREDICTION_CONFIG[\"probability_threshold\"]}')\n",
        "    \n",
        "    # Mark detected events\n",
        "    for event in events:\n",
        "        plt.axvspan(event['start_time'], event['end_time'], alpha=0.3, color='green')\n",
        "    \n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Anemonefish Call Detection Results: {Path(audio_filename).name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot binary detection\n",
        "    plt.subplot(2, 1, 2)\n",
        "    binary_preds = predictions > PREDICTION_CONFIG['probability_threshold']\n",
        "    plt.plot(time_points, binary_preds.astype(int), 'g-', linewidth=2, label='Detections')\n",
        "    \n",
        "    # Mark events\n",
        "    for i, event in enumerate(events):\n",
        "        plt.axvspan(event['start_time'], event['end_time'], alpha=0.5, color='green')\n",
        "        # Add event labels\n",
        "        mid_time = (event['start_time'] + event['end_time']) / 2\n",
        "        plt.text(mid_time, 0.5, f'Event {i+1}', ha='center', va='center', \n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    plt.ylabel('Detection')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylim(-0.1, 1.1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot\n",
        "    base_name = Path(audio_filename).stem\n",
        "    plot_path = output_dir / f\"{base_name}_timeline.png\"\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Timeline plot saved to: {plot_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Main Processing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_audio_file(audio_path, model, output_dir):\n",
        "    \"\"\"\n",
        "    Main processing pipeline for a single audio file.\n",
        "    \n",
        "    Args:\n",
        "        audio_path: Path to audio file\n",
        "        model: Loaded Keras model\n",
        "        output_dir: Output directory for results\n",
        "        \n",
        "    Returns:\n",
        "        processing_results: Dictionary with processing results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {audio_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # 1. Load and preprocess audio\n",
        "    audio_data, sample_rate, duration = load_and_preprocess_audio(\n",
        "        audio_path, SPECTROGRAM_CONFIG['target_sr']\n",
        "    )\n",
        "    \n",
        "    if audio_data is None:\n",
        "        print(f\"Failed to load audio file: {audio_path}\")\n",
        "        return None\n",
        "    \n",
        "    # 2. Generate sliding windows\n",
        "    windows = generate_sliding_windows(\n",
        "        audio_data, sample_rate,\n",
        "        WINDOW_CONFIG['window_duration'],\n",
        "        WINDOW_CONFIG['stride_duration']\n",
        "    )\n",
        "    \n",
        "    if not windows:\n",
        "        print(\"No windows generated\")\n",
        "        return None\n",
        "    \n",
        "    # 3. Create spectrograms\n",
        "    spectrograms, timestamps = process_windows_to_spectrograms(\n",
        "        audio_data, windows, sample_rate, SPECTROGRAM_CONFIG\n",
        "    )\n",
        "    \n",
        "    if len(spectrograms) == 0:\n",
        "        print(\"No spectrograms created\")\n",
        "        return None\n",
        "    \n",
        "    # 4. Run model predictions\n",
        "    predictions = predict_batch(\n",
        "        model, spectrograms, PREDICTION_CONFIG['batch_size']\n",
        "    )\n",
        "    \n",
        "    if len(predictions) == 0:\n",
        "        print(\"No predictions generated\")\n",
        "        return None\n",
        "    \n",
        "    # 5. Apply smoothing\n",
        "    smoothed_predictions = smooth_predictions(\n",
        "        predictions, PREDICTION_CONFIG['smoothing_window']\n",
        "    )\n",
        "    \n",
        "    # 6. Detect events\n",
        "    events = detect_events(smoothed_predictions, timestamps, PREDICTION_CONFIG)\n",
        "    \n",
        "    # 7. Save results\n",
        "    audio_filename = Path(audio_path).name\n",
        "    result_files = save_results(events, smoothed_predictions, timestamps, \n",
        "                               audio_filename, output_dir)\n",
        "    \n",
        "    # 8. Create visualization\n",
        "    plot_prediction_timeline(smoothed_predictions, timestamps, events, \n",
        "                           audio_filename, output_dir)\n",
        "    \n",
        "    # 9. Summary\n",
        "    processing_results = {\n",
        "        'audio_file': audio_filename,\n",
        "        'duration': duration,\n",
        "        'sample_rate': sample_rate,\n",
        "        'total_windows': len(predictions),\n",
        "        'total_events': len(events),\n",
        "        'events': events,\n",
        "        'result_files': result_files\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROCESSING COMPLETE: {audio_filename}\")\n",
        "    print(f\"Duration: {duration:.2f}s\")\n",
        "    print(f\"Windows processed: {len(predictions)}\")\n",
        "    print(f\"Events detected: {len(events)}\")\n",
        "    if events:\n",
        "        total_event_duration = sum(event['duration'] for event in events)\n",
        "        print(f\"Total event duration: {total_event_duration:.2f}s\")\n",
        "        print(f\"Coverage: {total_event_duration/duration*100:.1f}%\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return processing_results\n",
        "\n",
        "\n",
        "def process_multiple_files(input_dir, model, output_dir):\n",
        "    \"\"\"\n",
        "    Process multiple audio files in a directory.\n",
        "    \n",
        "    Args:\n",
        "        input_dir: Directory containing audio files\n",
        "        model: Loaded model\n",
        "        output_dir: Output directory\n",
        "        \n",
        "    Returns:\n",
        "        all_results: List of processing results\n",
        "    \"\"\"\n",
        "    # Find audio files\n",
        "    audio_extensions = ['.wav', '.WAV', '.mp3', '.MP3', '.flac', '.FLAC']\n",
        "    audio_files = []\n",
        "    \n",
        "    for ext in audio_extensions:\n",
        "        files = list(input_dir.glob(f'*{ext}'))\n",
        "        # Filter out hidden files (starting with .)\n",
        "        files = [f for f in files if not f.name.startswith('.')]\n",
        "        audio_files.extend(files)\n",
        "    \n",
        "    if not audio_files:\n",
        "        print(f\"No audio files found in {input_dir}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Found {len(audio_files)} audio files to process\")\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    for i, audio_file in enumerate(audio_files, 1):\n",
        "        print(f\"\\n[{i}/{len(audio_files)}] Processing: {audio_file.name}\")\n",
        "        \n",
        "        try:\n",
        "            result = process_audio_file(audio_file, model, output_dir)\n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_file}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Create overall summary\n",
        "    if all_results:\n",
        "        total_files = len(all_results)\n",
        "        total_events = sum(r['total_events'] for r in all_results)\n",
        "        total_duration = sum(r['duration'] for r in all_results)\n",
        "        \n",
        "        summary = {\n",
        "            'total_files_processed': total_files,\n",
        "            'total_audio_duration': total_duration,\n",
        "            'total_events_detected': total_events,\n",
        "            'files': all_results\n",
        "        }\n",
        "        \n",
        "        summary_path = output_dir / 'batch_processing_summary.json'\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"BATCH PROCESSING COMPLETE\")\n",
        "        print(f\"Files processed: {total_files}\")\n",
        "        print(f\"Total duration: {total_duration:.2f}s ({total_duration/60:.1f}min)\")\n",
        "        print(f\"Total events detected: {total_events}\")\n",
        "        print(f\"Batch summary saved to: {summary_path}\")\n",
        "        print(f\"{'='*80}\")\n",
        "    \n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Load Model and Run Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading trained model...\n",
            "Model loaded successfully from: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/models/binary_classifier/checkpoints_run_6/best_model.keras\n",
            "Model input shape: (None, 256, 256, 3)\n",
            "Model output shape: (None, 1)\n",
            "Model loaded successfully!\n",
            "Ready to process 1 audio files\n",
            "Files found:\n",
            "  - 20230321_010005_First_B48_M_NR_with labels.wav\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "print(\"Loading trained model...\")\n",
        "model = load_trained_model(MODEL_PATH)\n",
        "\n",
        "if model is None:\n",
        "    print(\"Failed to load model. Please check the MODEL_PATH in configuration.\")\n",
        "else:\n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    # Check if input directory has files\n",
        "    if not AUDIO_INPUT_DIR.exists():\n",
        "        print(f\"Input directory does not exist: {AUDIO_INPUT_DIR}\")\n",
        "        print(\"Please create the directory and add your audio files.\")\n",
        "    else:\n",
        "        # Find audio files in input directory\n",
        "        audio_files = [f for f in list(AUDIO_INPUT_DIR.glob('*.wav')) + list(AUDIO_INPUT_DIR.glob('*.WAV')) if not f.name.startswith('.')]\n",
        "        \n",
        "        if not audio_files:\n",
        "            print(f\"No audio files found in {AUDIO_INPUT_DIR}\")\n",
        "            print(\"Please add .wav files to the input directory.\")\n",
        "            print(f\"You can also modify AUDIO_INPUT_DIR in the configuration to point to your audio files.\")\n",
        "        else:\n",
        "            print(f\"Ready to process {len(audio_files)} audio files\")\n",
        "            print(\"Files found:\")\n",
        "            for f in audio_files:\n",
        "                print(f\"  - {f.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Process Audio Files\n",
        "\n",
        "Run this cell to process all audio files in the input directory:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 audio files to process\n",
            "\n",
            "[1/2] Processing: 20230321_010005_First_B48_M_NR_with labels.wav\n",
            "\n",
            "============================================================\n",
            "Processing: /Volumes/InsightML/NAS/3_Lucia_Yllan/Clown_Fish_Acoustics/data/prediction_input/20230321_010005_First_B48_M_NR_with labels.wav\n",
            "============================================================\n",
            "Loaded audio: 768.01s at 48000Hz\n",
            "Generated 7671 windows with 1.00s window, 0.10s stride\n",
            "Processing 7671 windows to spectrograms...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 6166/7671 [02:13<00:30, 48.61it/s]"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Process all audio files\n",
        "if 'model' in locals() and model is not None:\n",
        "    # Process all files in the input directory\n",
        "    results = process_multiple_files(AUDIO_INPUT_DIR, model, RESULTS_OUTPUT_DIR)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\nProcessing completed successfully!\")\n",
        "        print(f\"Results saved to: {RESULTS_OUTPUT_DIR}\")\n",
        "        \n",
        "        # Show brief summary\n",
        "        print(\"\\nBrief Summary:\")\n",
        "        for result in results:\n",
        "            print(f\"  {result['audio_file']}: {result['total_events']} events detected\")\n",
        "    else:\n",
        "        print(\"No files were processed successfully.\")\n",
        "else:\n",
        "    print(\"Model not loaded. Please run the previous cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. Individual File Processing (Optional)\n",
        "\n",
        "Use this section to process a single file for testing or detailed analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a single file for testing\n",
        "# Uncomment and modify the path below to process a specific file\n",
        "\n",
        "# SINGLE_FILE_PATH = \"/path/to/your/test/audio/file.wav\"\n",
        "# \n",
        "# if 'model' in locals() and model is not None:\n",
        "#     from pathlib import Path\n",
        "#     \n",
        "#     test_file = Path(SINGLE_FILE_PATH)\n",
        "#     if test_file.exists():\n",
        "#         print(f\"Processing single file: {test_file}\")\n",
        "#         result = process_audio_file(test_file, model, RESULTS_OUTPUT_DIR)\n",
        "#         \n",
        "#         if result:\n",
        "#             print(f\"Processing complete!\")\n",
        "#             print(f\"Events detected: {result['total_events']}\")\n",
        "#         else:\n",
        "#             print(\"Processing failed.\")\n",
        "#     else:\n",
        "#         print(f\"File not found: {test_file}\")\n",
        "# else:\n",
        "#     print(\"Model not loaded.\")\n",
        "\n",
        "print(\"To process a single file, uncomment and modify the code above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This prediction module provides a complete pipeline for detecting anemonefish calls in audio recordings:\n",
        "\n",
        "### **Key Features:**\n",
        "- **Training-Consistent Preprocessing**: Exact replication of training spectrogram generation\n",
        "- **Efficient Sliding Window**: In-memory processing with configurable overlap\n",
        "- **Batch Processing**: Handles multiple files with progress tracking\n",
        "- **Post-Processing**: Smoothing and intelligent event detection\n",
        "- **Multiple Output Formats**: CSV, JSON, and Audacity-compatible labels\n",
        "- **Visualization**: Timeline plots showing prediction confidence and detected events\n",
        "\n",
        "### **Output Files Generated:**\n",
        "- `{filename}_events.csv`: Detected events with timestamps and confidence scores\n",
        "- `{filename}_audacity_labels.txt`: Labels that can be imported into Audacity\n",
        "- `{filename}_predictions.csv`: All prediction scores for detailed analysis\n",
        "- `{filename}_summary.json`: Processing summary and metadata\n",
        "- `{filename}_timeline.png`: Visualization of detection results\n",
        "\n",
        "### **Configuration Options:**\n",
        "- **Window Duration**: 1.0 seconds (matches training)\n",
        "- **Stride Duration**: 0.1 seconds (configurable for temporal resolution)\n",
        "- **Probability Threshold**: 0.5 (adjustable for sensitivity)\n",
        "- **Smoothing**: Median filtering to reduce noise\n",
        "- **Event Merging**: Intelligent consolidation of nearby detections\n",
        "\n",
        "### **Usage:**\n",
        "1. Place audio files in the configured input directory\n",
        "2. Run the processing cells in sequence\n",
        "3. Results will be saved to the output directory with multiple format options\n",
        "4. Review timeline visualizations to assess detection quality\n",
        "\n",
        "The module is designed to be robust, efficient, and provide actionable results for acoustic monitoring of anemonefish populations.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "anemonefish_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
